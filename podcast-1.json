{"podcast_details": {"podcast_title": "IRL: Online Life Is Real Life", "episode_title": "The AI Medicine Cabinet", "episode_image": "https://image.simplecastcdn.com/images/7dfa8d26-0674-443c-944c-bfd6457bf0cf/bf1a36b3-b6f9-48b0-8c5a-7f7559bc12ea/3000x3000/irl-s6-ai-in-real-life-square-series-level-artwork.jpg?aid=rss_feed", "episode_transcript": " You gotta love the original Star Trek TV show from the 60s with its utopian futuristic spaceship. Remember how Dr. McCoy had that device with the flashing lights that could diagnose any medical condition? I wish we had this in real life. I'm a huge fan of sci-fi. But I wonder if sometimes we get too swept away by the promise of some technologies to help doctors and patients. AI is unlocking opportunities in the medical field for diagnostics, discoveries, and even treatments. But in a world where access to healthcare is anything but equal, how can AI developers build healthier systems and datasets for healthier people? This is a synthetic voice of a new AI chatbot in Rwanda created by a local team of speech tech developers. It gives millions of people access to vital information about COVID-19 over the phone. We'll hear more about this in a bit. I'm Bridget Todd, and this is IRL, an original podcast from Mozilla, the nonprofit behind Firefox. This season, it's all about AI. Today we're talking to AI innovators about life, death, and data. Specifically, personal data that should be private and donated data that could save lives. Before we begin, a trigger warning. We're starting with the story of someone who lost a loved one to cancer. I met Latoya. We married in 2008, and we were together for a little over a year or so, married. His name is Avery Smith. He's a software engineer in Tacoma Park, Maryland. His wife, Latoya, was a doctor, a podiatrist. One day, after they'd been married for about a year, she found a strange raised mole on her scalp. It turned out it was melanoma, a serious form of skin cancer. At the time, general understanding was that, oh, Black people really don't get melanoma. So that was an untruth. And then going through that process, it was very challenging because a lot of the statistics and protocols and services available and doctors that you interact with and everything made it seem like it was a very novel case of which it was. After the diagnosis, my wife, she passed away 18 months later. Here's the thing. For generations, medical textbooks mostly did not include images of a variety of skin tones, and they still don't for the most part. If you're searching for skin conditions on Google, you'll notice this too. White skin is the typical reference. I have a darker skin tone, so I have definitely experienced this firsthand. And in fact, I almost spent a ton of money on an expensive bed bug treatment after unsuccessfully searching things like dark skin, skin rash on Google. And after all that, it turned out to just be poison ivy. There's a lot of racial disparity in dermatology when it comes to quality of care. And it shows in skin cancer survival rates too. It can seem like it's a hopeless situation because people are just unfamiliar with what it is that you're going through. In the years following Latoya's death in 2011, an idea began to take shape for Avery. He started imagining an app where Black people could photograph a skin problem and have it automatically diagnosed. He came across a research paper in 2017 by someone who was working on an image recognition idea similar to his. Well, to an extent. The difference was that that paper came out. They made no mention about the variety of different skin types and particularly black skin. It made no mention of that. So I called them. I got the person on the phone and then I asked them the question. I said, hey, I think what you're doing is great. Is it a consideration for black skin? His response was, well, yeah, we like to do this for all skin types, which was OK. But what that basically told me was that the answer was no, it doesn't do it for black skin. And I was like, OK, well, why doesn't their research paper point that out? It doesn't. It just kind of ignores it. So I thought it was a bit misleading. Through their references, Avery discovered a project to create a very large repository of photos of skin problems as a resource for machine learning. The resource, which was referred to as like this chief resource, did not contain much of any black skin inside of it. It was like virtually none at all. We're talking tens of thousands of images. And they told me straight up, they said, hey, we know we're lacking in this area. If you would like to help us, we more than appreciate your contributions. And so that let me know. I said, OK, hey, I think I'm on to something. So that's how I got into the concept of image recognition, machine learning, and dermatology for black skin. Avery co-wrote a report on his research, which has been frequently cited. And he also started his own company, Mellalogic. Because there are incredibly few black dermatologists in the US and around the world for that matter, Avery is using AI to create a shortcut for people to get answers. Mellalogic users can upload photos of their skin to receive feedback from real doctors. In exchange, Avery asks them for a data donation. Their photos are anonymized, labeled, and added to a repository that will be used as training data for machine learning that works specifically for black people. Avery knows that there are giant companies working on similar diagnostic tools. But he believes he will earn the trust and participation of his community. They got more money than me, Google, Johnson & Johnson, all these other companies. They got more money than I do, but they don't have my story. That's one thing. They did not lose their wife to skin cancer. In addition to it, those companies are not necessarily run by people who are affected by this problem. Solving racial disparity in health care is core to Avery's mission. And it impacts how he builds technology and how he will measure success. Eventually, he aims to offer automatic diagnosis of skin conditions through a mobile phone, not just for cancer, but for thousands of other skin conditions too. It's not just a color thing. It's not just a DNA thing. But do I understand the specific hurdles, problems that we deal with on a regular basis? And then how can those things be overcome through technology? And I happen to exist at a junction between being a caregiver, software engineering, and also having black skin. And so being at the nucleus of all of that helps me be able to kind of put together solutions that something like Google or Johnson & Johnson, they just couldn't because they don't have that perspective and they're trying to appease so many different groups of people. It's still early days for Mellologic. Avery recently joined a local startup accelerator program for purpose-driven companies called Conscious Venture Labs. Meanwhile, AI tools for diagnosis are already in use around the world for dermatology and other medical branches, with not much attention paid to the bias that exists in datasets. Even when human life is at stake, there is no guarantee that an AI bias will be acknowledged or clearly communicated. Somebody can say, well, why not all skin types? Well, this goes back to the bias that I was talking about is that when you focus on everything, you're really not that good at nothing. Let's beam ourselves back a couple of years. In the early days of the pandemic, software developers around the world were joining forces to think about what they could do to help save lives, including in Rwanda. Local developers understand actually real problems in their communities. Remy Muhire belongs to a community of open source developers in Rwanda. They work on software for underrepresented African languages. The group is called Digital Umuganda. Their idea at the start of the pandemic was to use AI to solve a critical problem, access to public health information. Remy describes how the country's central health agency, the Rwanda Biomedical Center, or RBC for short, was overrun with phone calls. The call center were actually hit over 2000 call per day, which was actually a very big issue because there were actually quite a lot of restrictions and lockdowns and people couldn't actually move, get away from their homes. And people were always actually curiously calling, so when will be the reopenings and all those? Internet connectivity is low in Rwanda, and the RBC had no capacity to deal with that volume of calls. Together, they developed an AI chatbot that works in Kenyawanda, English, and French. It's called Mbaza, which means ask me in Kenyawanda. Basically you can make a phone call, like on the Rwanda Biomedical Center 114, so it's a hotline, and then have the bots interacting with you. So you can also send a message over WhatsApp or SMS, two-way interactions, or telegram. That's where the conversational AI messaging part comes in. And so you can also use like, U.S.S.D., so which are short codes where people dial. One of the lovely parts of Mbaza, it's more of like, you have people in rural areas, and we can just make phone calls because they're not text savvy, they're not digitally trained as well, and then just with a phone call they can get assistance. Since Mbaza launched nearly two years ago, it's been used by more than two million people. You can ask it questions about COVID infection rates and more. People in rural areas use the service to check on their vaccination status and also to check on their COVID results. There's actually restrictions for people to get in public places in Rwanda. If they are not fully vaccinated, if there's mass gathering like weddings or funerals, you need actually a COVID test to attend. So that's some of the use case like people in remote areas use to access Mbaza. Remy offers a basic explanation of how it works. The bot will have first to transcribe the voice into text. And then I pick actually the keywords and then send it to retrieve information regarding actually number of cases and like the current dates. And then those information will come as a text as well. And then the bot will have actually now to read those texts to the listener. Mbaza came together quickly, but years of work had gone into creating a voice data set for Kenya Rwanda. That's a language spoken by 12 million people in Rwanda. For two years, Remy was a community leader on a project to crowdsource voices through Mozilla's open source platform Common Voice. Data sets in more than 100 different languages can be used by anyone to train machine learning models for voice applications. This opens up new opportunities, especially for languages that aren't served by big tech, many of which are spoken by millions of people all over the world. Remy calls open source software and data like this digital infrastructure. It empowers local developers to create solutions to local problems, like access to public health information for people who can't read or don't have access to internet. With more infrastructure, there can be more innovation. So how do you build services where everyone can actually have access to it? So we have Mbaza as an example. And then from Mbaza and especially the work we did at Mozilla, the language infrastructure, the Kenya Rwanda language model, which is open source. I think the impact is actually yet to come. And we have actually more years to come. And this will be like phenomenal. Healthcare is not a technological problem, right? Healthcare is a social problem. It's a development problem. So when you're trying to fit a technology solution there, you really need to find what the problem is. Radhika Radhakrishnan is a scholar from Bangalore, India. She started in computer science engineering. Then she pivoted to feminist techno science studies. In her research, Radhika has focused on AI and data governance. I really began critically interrogating the technologies that I had been working on, that I had been building for many years before then. And it opened up this whole new world for me because I realized we were, as engineers, not really understanding the social implications of the tools we were building in the labs, not understanding how people were impacted by it. Radhika describes the AI and healthcare space in India as vast. She says that it often involves partnerships between Indian hospitals and multinational corporations like Google, IBM, and Microsoft. A product of these partnerships is diagnostic tools for people living in remote regions. I have been living in Bangalore for a long time, which is the IT capital of India. And a lot of startups around me were working on these AI healthcare technologies. So I thought, you know, I think that's what I should focus on next. I did my master's thesis on AI and healthcare in India. And the field work for that took me to places all around the country. And the stuff I saw there got me really worried because people weren't talking about that side of AI. We were only talking about how it could be used for amazing things, how it was the next best thing to happen to humanity, you know, AI for social good. But what I found was quite problematic. And that's where this journey began for me in this space. Radhika says diagnostic systems are widely used to describe medical problems, to predict medical issues in patients, and even to prescribe treatments or medicines. But she identified flaws in how they are devised and deployed. The problem that's been identified by the tech companies is we've got rural areas that don't have access to healthcare. And the problem is that you don't have enough doctors in the country to go and sit in the healthcare centers in these remote areas. And the ideal solution should be, hey, let's get more doctors to these places that don't have enough doctors, right? But what the tech companies are doing is sort of completely sidestepping that altogether and saying, let's introduce some kind of technology that's powered by AI that will therefore allow us to make these diagnosis from a distance without so much reliance on doctor being physically present. Small clinics in regions surrounding the hospital scanned the eyes and bodies of patients with different diagnostic tools. They sent thousands of images back and forth between cities and villages. Radhika was disturbed by how often doctors would refer to patients as data points rather than as people. Algorithmic systems were tested directly on patients. She saw a conflict of interest between the commercial interests of tech companies and the human interests of patients. People in villages were forced to sign consent forms they sometimes couldn't even read. It was either that or receive no treatment. A lot of the doctors, they told me, no, see, everything's going perfectly fine because nobody's complaining. The patients are not resisting it. They are not opposing it. And so what's really happening is that this lack of a choice is translating to a lack of any kind of opposition to the data collection. And that is quite conveniently construed as consent to the use of these technologies. Whereas at the end of the day, the patients themselves don't have much of an idea about what's happening with their data. Why is their data being collected? Because traditionally, you wouldn't need to scan a particular body part for the diagnosis that they are going in for because you're using an AI enabled image scanning algorithm. They are just trusting the system. And that's sad because that brings you to the next issue that is happening on the ground with patients, which is a lack of accountability. Radhika says designers of the systems always insist they have good intentions. Her concern is that they misrepresent business opportunities as AI for social good. She saw evidence of systems being deployed without testing and public documentation or safety nets. None of the medical practitioners I spoke to had any kind of consensus on questions about what will really happen if there is an error in the diagnosis from the system. When Radhika interviewed tech workers about ethics and consent, they passed the responsibility on to hospitals. It reflects a massive evasion of ethical responsibility towards patients. And that is putting people at the risk of untested technologies and not really understanding what their experiences are, not really even asking them whether they are okay with it. One of the medical practitioners actually told me that they don't even bother to explain to the rural patients what data is being collected, why it's collected, what they're doing with it because they blatantly said that these patients are not going to understand this massive infantilization of an entire population. And as a result of it, hindering their own ability to understand their diagnosis. It's widespread. It's horrendous. And that's the tool we're getting at the end of the day. That's our AI for social good on the ground. Radhika says regulation could help establish who would be held responsible if there was an error in the system. For example, an incorrect diagnosis. It could also help establish a process for people to seek recourse. These could also guide procedures for getting consent from patients. And you could provide and explain meaningful health care alternatives if they say no. Radhika says many regulations focus on just the commercial deployment of tech products, but that the risk for harm is just as serious during data collection and testing. That's a gray area. All of these companies are functioning in right now. The tools are not technically deployed yet. They're still being tested. And there's not a lot of regulation that's looking at how to make sure that that testing itself is being done ethically. One of Radhika's key recommendations is that hospital treatment facilities should separate from algorithmic testing facilities. But that doesn't mean she's completely against AI and health care. It often comes across as anyone who's been critical of some of these applications is critical of the technology itself, which is not true. There are definitely ways of doing this better. And I think the whole goal of even doing research of this kind is to show that there's a way to do this better. And then if we do it better, everyone benefits. So much of our technology, especially when it comes to health, purports to help us be our best selves. For instance, have you ever felt depressed and wondered, maybe there's an app for that? A lot of people have. Mental health apps have boomed in the past years. Many use AI to chat with you, to keep you engaged, and to analyze your words and behaviors. It doesn't play a part in all of the apps. It only plays a part in some of them, from what we can tell. Jen Kaltreiter is the lead investigator for Mozilla's Privacy Not Included Shopper's Guide. This year, the team reviewed 32 popular mental health and prayer apps. They wanted to see how the apps stand up to scrutiny on privacy, security, and AI transparency. One of the things that we found is it's really, really hard to tell what's actually going on with the AI in these consumer products, because the AIs they use are often proprietary. And so a number of companies don't share what's going on, because that's their business. Jen says there are AI chatbots and other natural language processing techniques that are used to listen and respond to users in order to gauge their moods and keep them engaged for longer. She says mental health apps have some of the worst privacy and security practices she has ever seen. It doesn't matter if it's a free app or an app you've paid for. Neither is guaranteed to keep your secrets. What we learned was they're collecting a lot of data, creepy data that maybe you don't want collected. They're treating it like a business asset, having a company know that and share that information with a marketing agency that could then be used by some really bad people with questionable ideologies to target you, to move you towards a hateful mentality or something like that, because they know you're vulnerable. It gets really scary. When it comes to mental health, everyone is vulnerable. Maybe you don't want the world to know that you're gay and you're depressed, and that's something that could live on forever and somebody could use that to target you with conversion therapy. Because a lot of these privacy policies will say, we'll never share or sell your data without your consent. And you're like, great, I'm protected. But what does consent look like? Sometimes in a privacy policy, you'll see that it says once you've downloaded and registered with our app, you've given us consent to use your personal information in the ways we articulate in this privacy policy. So people might not understand when they're really struggling that by downloading and installing an app and using it, they've suddenly given up consent for their information to be shared for interest-based advertising or sold to data brokers or used for ad marketing for this company to try and get more people to use the app. There was one app that impressed the Privacy Not Included team. It's called WISE. It's an AI chat bot founded by a woman who I believe is from India, who was, from what I read, was depressed and discovered that she preferred talking to a bot more than a human. And so she built this company. And just reading their privacy policy, it was just different. They don't require that you share personal information to use the app. And you can just use a nickname. And then they also talk about how they aren't going to request your personal data. If you accidentally submit it, they have ways that they can process it. And they say irreversibly redact any personal information within 24 hours. As consumers, it's hard to know who to trust. With AI in particular, we're promised great services and don't necessarily notice our data changing hands. I believe that there should be standards for apps that use AI to protect against bias and to provide transparency. Absolutely, yes. Is that going to happen anytime soon? I don't know. It doesn't seem that way. Just way too often, we just can't tell. We can't tell if it's trustworthy. We can't tell exactly how it's being used. We can't tell if there's bias in it. We can't tell if a user has that much control over it. And that's unfortunate because that's what we want to see in the world. When is the AI in the medicine cabinet trustworthy? And when is it snake oil? In the rush to automate more and more, there are huge risks of bias and surveillance. There is rarely enough transparency and accountability. Who has the power over AI? Who is shifting that power? We've heard from champions all season who insist there's a better way to build, deploy, and understand these technologies. Artificial intelligence has captured the world's imagination. But let's not look to algorithms for all the answers. This season showed that the intelligence in AI will come from all of us. We decide how to use AI, and we can demand that it be trustworthy. I'm Brigitte Todd, and that wraps up season 6 of IRL, an original podcast from Mozilla, the nonprofit behind Firefox. Before I go, check out InternetHealthReport.org for extended interviews and even photos of everyone you heard from this season. We'd love to hear your thoughts. Did we change your mind about anything? Let us know and stay tuned."}, "podcast_summary": "KEY TAKEAWAYS:\n\n- Lack of diversity in medical textbooks and image recognition models leads to racial disparity in dermatology and skin cancer survival rates.\n- AI can help address this issue by creating diagnostic tools specifically for black skin, such as Mellalogic's app that allows users to photograph a skin problem and receive feedback from real doctors.\n- Developers must ensure that AI models for healthcare have diverse datasets and address biases to ensure equitable and accurate diagnoses.\n- In Rwanda, AI chatbot Mbaza provides vital COVID-19 information to millions of people through phone calls, WhatsApp, SMS, and other messaging platforms.\n- Open source platforms like Mozilla's Common Voice enable the creation of voice data sets in underrepresented languages, allowing for innovative solutions to local problems and improved access to public health information.", "podcast_guest": "Avery Smith", "podcast_highlights": "Summary:\nThis podcast episode explores the use of AI in medicine and the challenges and opportunities it presents. It highlights the importance of building healthier systems and datasets for all people, addressing racial disparities in healthcare, and ensuring ethical practices and accountability in AI technology. The episode features stories of individuals using AI to improve dermatology care for black patients, provide access to public health information in Rwanda, enhance mental health apps, and offers insights and critical questions on the topic.\n\nKey insights:\n1. Racial disparities in healthcare and the lack of representation in datasets and medical textbooks need to be addressed in AI applications in medicine.\n2. AI has the potential to improve access to healthcare and provide solutions for specific communities and their unique needs.\n3. Open-source infrastructure and data can empower local developers to create solutions for local problems, such as language accessibility in healthcare.\n4. The deployment of AI tools in healthcare should prioritize patient welfare, informed consent, transparency, and accountability.\n5. Privacy and security concerns arise with the collection and use of personal data by AI-powered mental health apps, highlighting the need for better regulation and user control.\n\nCritical questions:\n1. How can AI developers ensure diversity and inclusivity in datasets and algorithms to address racial disparities in healthcare?\n2. What measures can be taken to ensure ethical practices and accountability in AI diagnostics and treatment tools?\n3. How can access to healthcare information and services be improved for underserved communities using AI technology?\n4. How can regulations be implemented to protect user privacy and prevent the misuse of personal data in AI-powered mental health apps?\n5. What steps can healthcare providers and technology companies take to build trust and address concerns regarding bias and surveillance in AI applications in medicine?"}